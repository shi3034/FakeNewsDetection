{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load dataset\n",
        "def load_dataset(filepath):\n",
        "    return pd.read_csv(filepath, sep='\\t', on_bad_lines='skip')\n",
        "\n",
        "# Text cleaning and preprocessing\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove non-alphabetical characters\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    text = text.strip()  # Remove leading/trailing whitespace\n",
        "    return text\n",
        "\n",
        "# Extract features\n",
        "def extract_features(df):\n",
        "    sia = SentimentIntensityAnalyzer()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    # Title length and text length\n",
        "    df['title_length'] = df['title'].apply(lambda x: len(x.split()))\n",
        "    df['text_length'] = df['text'].apply(lambda x: len(x.split()))\n",
        "\n",
        "    # Keyword density\n",
        "    def keyword_density(text):\n",
        "        keywords = ['shocking', 'breaking', 'exclusive']\n",
        "        text_tokens = text.split()\n",
        "        return sum(1 for word in text_tokens if word in keywords) / len(text_tokens)\n",
        "\n",
        "    df['keyword_density'] = df['text'].apply(keyword_density)\n",
        "\n",
        "    # Sentiment analysis\n",
        "    df['title_sentiment'] = df['title'].apply(lambda x: sia.polarity_scores(x)['compound'])\n",
        "    df['text_sentiment'] = df['text'].apply(lambda x: sia.polarity_scores(x)['compound'])\n",
        "\n",
        "    # Removing stopwords\n",
        "    def remove_stopwords(text):\n",
        "        return ' '.join([word for word in text.split() if word not in stop_words])\n",
        "\n",
        "    df['cleaned_text'] = df['text'].apply(remove_stopwords)\n",
        "\n",
        "    return df\n",
        "\n",
        "# Prepare data for model training\n",
        "def prepare_data(df):\n",
        "    tfidf = TfidfVectorizer(max_features=5000)\n",
        "    X_text_features = tfidf.fit_transform(df['cleaned_text']).toarray()\n",
        "\n",
        "    # Combine other features\n",
        "    X_other_features = df[['title_length', 'text_length', 'keyword_density', 'title_sentiment', 'text_sentiment']].values\n",
        "    X = np.hstack([X_text_features, X_other_features])\n",
        "\n",
        "    y = df['label']\n",
        "\n",
        "    return X, y\n",
        "\n",
        "# Train model\n",
        "def train_model(X, y):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_proba = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    # Evaluation metrics\n",
        "    metrics = {\n",
        "        'accuracy': accuracy_score(y_test, y_pred),\n",
        "        'precision': precision_score(y_test, y_pred),\n",
        "        'recall': recall_score(y_test, y_pred),\n",
        "        'f1_score': f1_score(y_test, y_pred),\n",
        "        'roc_auc': roc_auc_score(y_test, y_proba)\n",
        "    }\n",
        "\n",
        "    print(\"Evaluation Metrics:\")\n",
        "    for metric, value in metrics.items():\n",
        "        print(f\"{metric.capitalize()}: {value:.2f}\")\n",
        "\n",
        "    return model, metrics\n",
        "\n",
        "# Save results\n",
        "def save_results(test_data, predictions, filepath):\n",
        "    results = pd.DataFrame({\n",
        "        'title': test_data['title'],\n",
        "        'predicted_label': predictions\n",
        "    })\n",
        "    results.to_csv(filepath, index=False)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_dataset_path = \"/content/train.tsv\"  # Path to training dataset\n",
        "    test_dataset_path = \"/content/test.tsv\"    # Path to test dataset\n",
        "    output_path = \"results.csv\"                      # File to save test predictions\n",
        "\n",
        "    # Load and preprocess training dataset\n",
        "    train_data = load_dataset(train_dataset_path)\n",
        "    train_data['title'] = train_data['title'].apply(clean_text)\n",
        "    train_data['text'] = train_data['text'].apply(clean_text)\n",
        "    train_data = extract_features(train_data)\n",
        "\n",
        "    # Prepare and train the model\n",
        "    X_train, y_train = prepare_data(train_data)\n",
        "    model, metrics = train_model(X_train, y_train)\n",
        "\n",
        "    print(\"Training Metrics:\", metrics)\n",
        "\n",
        "    # --- NEW SECTION: Test Data Handling ---\n",
        "\n",
        "    # Load and preprocess test dataset\n",
        "    test_data = load_dataset(test_dataset_path)\n",
        "    test_data['title'] = test_data['title'].apply(clean_text)\n",
        "    test_data['text'] = test_data['text'].apply(clean_text)\n",
        "    test_data = extract_features(test_data)\n",
        "\n",
        "    # Prepare test data for predictions\n",
        "    tfidf = TfidfVectorizer(max_features=5000)\n",
        "    X_test_text_features = tfidf.fit_transform(test_data['cleaned_text']).toarray()\n",
        "    X_test_other_features = test_data[['title_length', 'text_length', 'keyword_density',\n",
        "                                       'title_sentiment', 'text_sentiment']].values\n",
        "    X_test = np.hstack([X_test_text_features, X_test_other_features])\n",
        "\n",
        "    # Predict on test data\n",
        "    predictions = model.predict(X_test)\n",
        "\n",
        "    # Save predictions to a CSV file\n",
        "    save_results(test_data, predictions, output_path)\n",
        "\n",
        "    print(f\"Predictions saved to {output_path}\")\n",
        "\n",
        "\n",
        "def load_dataset(filepath):\n",
        "    try:\n",
        "        df = pd.read_csv(filepath, sep='\\t', on_bad_lines='skip')\n",
        "        df = df.dropna(subset=['label'])  # Drop rows with missing labels\n",
        "        df['label'] = df['label'].astype(int)  # Ensure labels are integers\n",
        "        return df\n",
        "    except pd.errors.ParserError as e:\n",
        "        print(\"ParserError encountered:\", e)\n",
        "        return None\n",
        "\n",
        "# Ensure no NaN values in the target column during preprocessing\n",
        "train_data = load_dataset(train_dataset_path)\n",
        "if train_data['label'].isna().any():\n",
        "    print(\"Warning: Found missing values in 'label'. Dropping such rows.\")\n",
        "    train_data = train_data.dropna(subset=['label'])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RS4Rue5YdbV_",
        "outputId": "50bf910b-d7c2-41d4-a165-67eeda3533f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Metrics:\n",
            "Accuracy: 0.99\n",
            "Precision: 0.99\n",
            "Recall: 0.99\n",
            "F1_score: 0.99\n",
            "Roc_auc: 1.00\n",
            "Training Metrics: {'accuracy': 0.9886666666666667, 'precision': 0.9901788846018941, 'recall': 0.986028641285365, 'f1_score': 0.9880994049702485, 'roc_auc': 0.9992637980250146}\n",
            "Predictions saved to results.csv\n"
          ]
        }
      ]
    }
  ]
}